{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPRExGWEQ+AjWowNO1yDIJF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Heebaashrafi/heeba-project/blob/main/HADOOP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtmVgcCbEKSC",
        "outputId": "cabb90ec-4876-4851-a6fa-27c911b2083d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.4.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import pyspark\n",
        "#initialize the spark\n",
        "spark=SparkSession.builder.appName(\"Example 1\").getOrCreate()\n",
        "data_df=spark.read.csv(\"stud.csv\")\n",
        "data_df.show()\n",
        "data_df.head()\n",
        "data_df.tail(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVPJ5pxJGRH8",
        "outputId": "0da22407-29ab-4cc1-a391-eeca86773d31"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+------+\n",
            "|    _c0|_c1|   _c2|\n",
            "+-------+---+------+\n",
            "|   Name|Age|Course|\n",
            "| Heeba1| 22|    IT|\n",
            "| Heeba2| 22|    IT|\n",
            "| Heeba3| 22|    IT|\n",
            "| Heeba4| 22|    IT|\n",
            "| Heeba5| 22|    IT|\n",
            "| Heeba6| 22|    IT|\n",
            "| Heeba7| 22|    IT|\n",
            "| Heeba8| 22|    IT|\n",
            "| Heeba9| 22|    IT|\n",
            "|Heeba10| 22|    IT|\n",
            "+-------+---+------+\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(_c0='Heeba6', _c1='22', _c2='IT'),\n",
              " Row(_c0='Heeba7', _c1='22', _c2='IT'),\n",
              " Row(_c0='Heeba8', _c1='22', _c2='IT'),\n",
              " Row(_c0='Heeba9', _c1='22', _c2='IT'),\n",
              " Row(_c0='Heeba10', _c1='22', _c2='IT')]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#joining two different data frames\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark\n",
        "#initialize the spark\n",
        "spark=SparkSession.builder.appName(\"Example 2\").getOrCreate()\n",
        "#create dataframe\n",
        "data1=[(\"zaid\",1),(\"dhwani\",2),(\"adam\",3),(\"heeba\",4)]\n",
        "data2=[(\"zaid\",\"engineer\"),(\"dhwani\",\"teacher\"),(\"adam\",\"student\"),(\"heeba\",\"CEO\")]\n",
        "col1=[\"NAME\",\"NO\"]\n",
        "col2=[\"NAME\",\"PROFESSION\"]\n",
        "df1=spark.createDataFrame(data1,col1)\n",
        "df2=spark.createDataFrame(data2,col2)\n",
        "\n",
        "#JOINING DATAFRAMES\n",
        "joined_df=df1.join(df2,\"NAME\",\"inner\")\n",
        "joined_df.show()\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xuryJ2P-JfaL",
        "outputId": "5160d778-95d3-49fd-dbf1-27c0cbf0a6c7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---+----------+\n",
            "|  NAME| NO|PROFESSION|\n",
            "+------+---+----------+\n",
            "|  adam|  3|   student|\n",
            "|dhwani|  2|   teacher|\n",
            "| heeba|  4|       CEO|\n",
            "|  zaid|  1|  engineer|\n",
            "+------+---+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import pyspark\n",
        "#initialize the spark\n",
        "spark=SparkSession.builder.appName(\"Example 3\").getOrCreate()\n",
        "data1=[(\"zaid\",1),(\"dhwani\",2),(\"adam\",3),(\"heeba\",4),(\"rahul\",5),(\"umang\",6)]\n",
        "data2=[(\"zaid\",\"engineer\"),(\"dhwani\",\"teacher\"),(\"adam\",\"student\"),(\"heeba\",\"engineer\"),(\"rahul\",\"student\"),(\"umang\",\"teacher\")]\n",
        "col1=[\"name\",\"no\"]\n",
        "col2=[\"name\",\"profession\"]\n",
        "\n",
        "df=spark.createDataFrame(data2,col2)\n",
        "group_df=df.groupBy(\"profession\").count()\n",
        "group_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1KyFuLlP95C",
        "outputId": "809db5ed-d974-47da-c86f-c83c3ae71d01"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+\n",
            "|profession|count|\n",
            "+----------+-----+\n",
            "|   teacher|    2|\n",
            "|   student|    2|\n",
            "|  engineer|    2|\n",
            "+----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import pyspark\n",
        "#initialize the spark\n",
        "spark=SparkSession.builder.appName(\"Example 4\").getOrCreate()\n",
        "#create dataframe\n",
        "data1=[(\"zaid\",1),(\"dhwani\",2),(\"adam\",3),(\"heeba\",4)]\n",
        "data2=[(\"zaid\",\"engineer\"),(\"dhwani\",\"teacher\"),(\"adam\",\"student\"),(\"heeba\",\"CEO\")]\n",
        "col1=[\"NAME\",\"NO\"]\n",
        "col2=[\"NAME\",\"PROFESSION\"]\n",
        "\n",
        "df1=spark.createDataFrame(data1,col1)\n",
        "df2=spark.createDataFrame(data2,col2)\n",
        "\n",
        "union_df=df1.union(df2)\n",
        "union_df.show()\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rd2AOCY0SjIB",
        "outputId": "a2fbbeb9-7f55-4410-b827-7810cb8910a3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+\n",
            "|  NAME|      NO|\n",
            "+------+--------+\n",
            "|  zaid|       1|\n",
            "|dhwani|       2|\n",
            "|  adam|       3|\n",
            "| heeba|       4|\n",
            "|  zaid|engineer|\n",
            "|dhwani| teacher|\n",
            "|  adam| student|\n",
            "| heeba|     CEO|\n",
            "+------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#write a program with function using a.collect,filter,map,mapreduce,reduce\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark\n",
        "#initialize the spark\n",
        "spark=SparkSession.builder.appName(\"Example 5\").getOrCreate()\n",
        "data=[1,2,3]\n",
        "rdd=spark.sparkContext.parallelize(data)\n",
        "collected_data=rdd.collect()\n",
        "print(\"Collect:\",collected_data)\n",
        "\n",
        "\n",
        "#filter\n",
        "filter_rdd=rdd.filter(lambda x:x%2==0)\n",
        "filter_data=filter_rdd.collect()\n",
        "print(\"Even:\",filter_data)\n",
        "\n",
        "#mapping\n",
        "#square of each value\n",
        "\n",
        "rdd=spark.sparkContext.parallelize(data)\n",
        "sq_rdd=rdd.map(lambda x: x**2)\n",
        "\n",
        "print(\"square:\", sq_data)\n",
        "\n",
        "#mapreduse\n",
        "rdd=spark.sparkContext.parallelize(data)\n",
        "sq_rdd=rdd.map(lambda x: x**2).reduce(lambda a,b:a+b)\n",
        "\n",
        "print(\"mapreduce: \",sq_rdd)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vA3w5NOETrrx",
        "outputId": "d7c77a17-a3dd-44bf-82ca-649ded3d6c9c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collect: [1, 2, 3]\n",
            "Even: [2]\n",
            "square: [1, 4, 9, 16, 25, 36, 49, 64, 81]\n",
            "mapreduce:  14\n"
          ]
        }
      ]
    }
  ]
}